---
title: AI学习记录随笔1-机器学习
date: 2018-01-27 22:27:12
tags:
mathjax: true
---

> 人工智能(AI) > 机器学习(ML) > 深度学习（DL）

深度学习是机器学习一种极为有效的办法，但不是全部。

# 一、概念 & 建议
## 1. 机器学习
在没有明确设置的情况下，使计算机具有学习能力的研究领域。

## 2. 机器学习的类型：
### a) 监督学习
让计算机学会某件事，例如给一个包含正确答案的数据集，然后计算机给出更多的答案。例如回归问题和分类问题：

**回归问题**  
对一堆数据集给出一个尽量拟合的曲线，然后给出要求数据的Y值。

**分类问题**  
对一堆拥有是否两个状态的已分类数据集进行分析，然后给出要求数据的状态。（预测离散值的输出）

### b) 无监督学习
给出一堆没有分类的数据集，让计算机给数据分簇、分类。例如鸡尾酒会算法对混合的声音进行分离。

### c) 还有对抗学习等其他的，这里没有过多介绍

## 3. 学习建议
利用 `octave` 和 `Matlab` 先对算法进行设计可以让新手更有学习效率。

# 二、监督学习——梯度下降算法

线性问题

## 1. 代价函数

![图例](AI学习记录随笔1-机器学习/代价函数.png)

以拟合一个一维线性函数（假设函数）为例：`Y = aX + b`，就是要通过算法找到最适合的 `a` 和 `b`，以使输入样本后能够得到最贴近结果的输出。这样就有一个衡量 `a`、`b` 符合度的代价函数，还是以上面为例，一维线性函数最常用的代价函数如下：

$$\frac{1}{2m}\sum_{i=1}^m(Y_i-y)^2$$

其中 `m` 表示样例个数，$Y_i$ 为所求的的结果，`y` 是训练集中的真实数据。可以把假设函数和代价函数的函数图像画出，得到一个比较直观的了解。

## 2. 最小化代价函数的常用算法————梯度下降算法
**梯度下降算法**常常用来最小化代价函数，其思想如下：

1. 对于代价函数 `J(x,y)` ，给定一个初始值 `(x,y)` ，这个初始值任意；
2. 不停改变 `x` 和 `y` ，以使 `J(x,y)` 变小，直到找到最小或局部最小值。

$$\theta_j:=\theta_j-\alpha\frac{\sigma}{\sigma\theta_j}J(\theta_0\theta_1)$$

$\alpha$ 表示学习率，即每一步改变的跨度，:= 表示赋值，$\frac{\sigma}{\sigma\theta_j}J(\theta_0\theta_1)$，是面的斜率。

值得注意的地方有两个：

- 对于两个参数的 $\theta_0$ 、$\theta_1$,更新一个时，要同时更新另一个
- 随着每一次迭代，斜率会变小，跨度也会变小，所以不需要一直修改变化率 $\alpha$

## 3. 多元线性回归

$$h_\theta=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3+\theta_4x_4$$

$$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_i^{(i)}$$

$\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_i^{(i)}$ 这一段表示的是 $\frac{\alpha}{\alpha\theta_j}J(\theta)$

## 4. 实用技巧1——特征缩放
实用梯度下降时，有时候需要进行很多次才能最终收敏，此时可以将特征值进行缩放，使其更快收敏。

$$\frac{x_1-平均}{max-min}$$

减去样本的平均值，然后除以，样本最大值和最小值的差。

## 5. 实用技巧2——学习率的选择
- 为了确定梯度下降是否正常工作，可以画出 $J_\theta$ 随迭代次数变化的曲线，来观察梯度下降是否正常工作。
- 可以通过观察曲线，来找到收敏的点
- 还可以通过判断 $J_\theta$ 小于某个值 $\alpha$ 来自动得到收敏的点，但 $\alpha$ 的选择是一个问题

## 6. 区别与迭代方式的正规方程解法——不需要特征缩放——适合特征参数数量较小的情况
通过使偏导数为 0 来最小化 $\theta$ 

$$\theta=(X^TX)^{-1}X^Ty$$

# 三、监督学习——Logistic分类算法

## 1. 二元分类
对于离散性质的分类问题，有 $y\in\{1,2\}$ 的二元分类问题和，$y\notin\{0,1,2,3\}$ 的多元分类问题。

$$\begin{cases}
h(\theta)=g(\theta^Tx)\\
g(z)=\frac{1}{1+e^{-z}}
\end{cases}$$

$$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$$

代价函数：
$$J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})$$
$$Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$$

化简代价函数：
$$J(\theta)=-\frac{1}{m}[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$

梯度下降：
$$\theta_j:=\theta_j-\alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y{(i)})x_j^{(i)}$$

## 2. 多元分类

假如有训练集有A、B、C三个类，可以先把B、C统一看成一类E，这样就得到了A、E的近似二元分类，然后在把A、B看成一类F，这样就得到了F、C的近似二元分类，类似的可以进行第三个分类。

# 四、过度拟合问题
数据虽然符合，但是曲线很扭曲，一般变量过多，但是训练数据比较少的时候会出现过度拟合。

## 1. 解决过度拟合的办法
1. 减少变量
2. 正则化:给予一定的惩罚，让相关参数不过多的受数据影响

## 2. 线性回归的正则化公式

$$J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2]$$

### a) 梯度下降
$$\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_i^{(i)}+\frac{\lambda}{m}\theta_j]$$

化简：
$$\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_i^{(i)}$$

$\lambda$：惩罚系数，如果过大会导致欠拟合。

### b) 正规

$$\theta=(X^TX+\lambda\begin{bmatrix}0&0&0\\0&1&0\\0&0&1\end{bmatrix})^{-1}.X^Ty$$

## 3. 逻辑回归的正则化公式

### a) 梯度下降
$$\theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]$$

### b) 高级优化



# 五、线性代数基本概念
## 1. 矩阵(Matrix)
- $A_ij$ 表示一个矩阵的第 i 行 j 列个元素
- $R^4\times 2$ 泛指一个 4 行 2 列的矩阵
- 一个矩阵的维数：$i \times j$
- 一般习惯用大写表示
- 单位矩阵 I，斜对角线的一条都是 1 的矩阵，单位矩阵乘以任何矩阵等于那个矩阵

## 2. 向量(Vector)
- 只有一列的矩阵
- 一般习惯用小写表示一个向量或标量

## 3. 矩阵的运算
### 加法

### 乘法
**矩阵与矩阵的乘法**：只有一个矩阵的列数等于第二个矩阵的行数时，两个矩阵才能相乘，可以简记为
```
(m x s)(s x n) = (m x n)
// 注意 AB != BA ，即不满足交换律
// 但是满足结合律 A x (B x C) = (A x B) x C
```

### 逆矩阵
- 只有 m x m 的矩阵(方阵)才有逆矩阵，全零矩阵也没有逆矩阵
- $ A \times A^{-1} = A^{-1} \times A = I $ (单位矩阵)


### 装置矩阵
- A 的装置矩阵是　$A^T$
- A 的每一行作为其装置矩阵的每一列

#